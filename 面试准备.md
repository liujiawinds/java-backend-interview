#面试准备

开场白：
大概讲一下上一份工作的经历。
我是在16年9月进入的maxent，进入公司的时候因为在性能优化和监控这一块比较有经验，leader就让我负责了之前性能存在很大问题的反欺诈SaaS平台的开发和维护工作。

反欺诈SaaS平台的主要功能就是对事件进行风险评分，分值越高，风险越高。流程是：先根据事件携带信息进行特征的抽取，然后把各种特征传入打分模型进行计算。

这里面就涉及到了两个核心模块，特征提取模块，模型打分模块。

特征提取模块在我接手的时候单机（8core 16G 微软云）性能是在80qps，latency在1500ms左右，后来经过我重构之后性能达到了单机（4core 8G 阿里云）约等于400qps，latency 3-400ms。
这个模块是io密集型的应用，负责从redis读取计数，和调用rpc接口获取计数的离群度。这个模块的瓶颈主要是在redis这个部分，redis的qps在2w（一个feature调用7次redis，总共有35个时间频次特征，每秒80qps，即**7x35x80**）的样子。

后来我就先把这个模块拆成了几个业务单元，然后每一个业务单元都是有自己独自的线程池。这样分离了cpu密集运算和io密集运算，每一个单元之间用queue连接，形成了一个流式结构，queue同时充当了buffer，然后将io密集的操作做成异步io，尽可能多的发起更多的io操作。

另外

1. 把redis cluster换成了sharded jedis，封装查询接口，通过pipeline实现异步批量查询。（batch: size 270左右，latency 24ms。扩展和HA这一块暂时还没有考虑过）
2. 把rpc 查询接口也封装成异步批量的方式。（batch: size 80左右，latency 18ms一个list装了很多请求，发到服务端，调用各个具体方法并按序存放响应list，返回响应list）
3. 把kafka producer替换成了异步批量方式（producer.type=async；batch.num.messages=100, queue.buffering.max.ms=10）

这样改造完了之后，qps在320的样子。不过latency在980ms左右。产品部门要求的latency在300ms以内。所以后来就再次进行改造。

后来采用了java8里面的CompletableFuture来重构异步的流转，进一步减少了io等待时间。qps也升到400多一点，latency 360ms （95%百分位）

现在这个架构还剩下的问题：allof 把所有feature的future都给捆绑在一起了。如果一个抛出异常那allof future也就会抛异常。这样做会导致整个事件的抽取失败。虽然现在业务上容忍了这一个情况，还是需要想办法去避免。

异步处理的核心就是queue；任务在queue间的流转，就是流处理；allof 树形的future管理结构

同时，因为调优的过程中需要不断的记录应用运行状态，我搭建了一个基于prometheus的监控系统。prometheus可以通过应用暴露出来的http接口，定时去获取metrics（jvm状态，主机状态，）并通过grafana展示出来。可以很详细的观察到具体指标的变化，用来指导调优过程和方向。


打分模块，后来就是照搬的特征抽取模块的架构，只是因为它是cpu密集性应用，所以对各个stage的线程数这些调整了一下。用到的一些算法都是比较基础的算法，比如逻辑回归（二项逻辑回归，多项逻辑回归）。


## Java基础

### 多态的实现原理
含义：抽象的来讲，多态的意思就是同一消息可以根据发送对象的不同而采用多种不同的行为方式。（发送消息就是函数调用）

~~~Java
Animal dog = new Dog();
Animal cat = new Cat();

dog.bark();
cat.bark();
~~~

实现的原理是动态绑定，程序调用的方法在运行期才动态绑定，追溯源码可以发现，JVM 通过参数的自动转型来找到合适的办法。

### String StringBuffer StringBuilder
StringBuffer是线程安全的，每次操作字符串，String会生成一个新的对象，而StringBuffer不会；StringBuilder是非线程安全的




### jvm 内存回收


### Java并发包

#### 线程间通信的方式
 
 1. wait/notify/notifyAll
 2. 管道
 3. join
 4. <del>ThreadLocal</del>（java并发编程艺术上面这么说，我认为它不是线程间的）
 5. synchronized/volatile
 
#### ReentrantLock
支持一个线程对资源的重复加锁（synchronized也隐式支持，可以用递归尝试）， 除此之外，该锁还支持获取锁的公平与非公平性选择。

公平锁往往没有非公平锁的性能高（非公平锁可以减少context switch），但是tps并不决定一切，在公平锁的情况下，等待时间越久的线程就会优先得到锁。
 
 ~~~Java
 ReentrantLock w = new ReentrantLock();
 w.lock();
 try{
    // some business code
 } finally {
 	w.unlock();
 }
 ~~~
 
1. Synchronized是在JVM层面上实现的，无需显示的加解锁，如果代码执行时出现异常会自动释放锁。而ReentrantLock需显示的加锁和释放锁；
2. Synchronized是针对一个对象的，而ReentrantLock是代码块层面的
3. RentrantLock 有公平和非公平的锁的机制
4. ReentrantReadWriteLock引入了读写和并发机制，可以实现更复杂的锁机制，并发性相对于ReentrantLock和Synchronized更高。
 
 
 
##### Semaphore(信号量)，countDownLatch，CyclicBarrier
CountDownLatch只能用一次，CyclicBarrier可以使用resetting方法重置。
CyclicBarrier还可以通过getNumberWaiting获取阻塞的线程数量，通过isBroken了解阻塞的线程是否被中断。

Semaphore用在资源有限的应用场景，比如数据库连接池。假如有一个需求，要读取几万个文件的数据，因为都是IO密集型人物，可以启动几十个线程并发的去读取，但是如果读到内存中之后，还要存储到数据库中，而数据库的连接只有10个，这时我们必须控制
 
 
##### ThreadLocal
线程局部变量是存储在Thread对象的 threadLocals 属性中，而 threadLocals 属性是一个 ThreadLocal.ThreadLocalMap 对象，这个ThreadLocalMap其实是一个Entry数组，偶数位是key，奇数位是value。

即：每一个线程都有一个Entry数组来保存线程局部变量，key是new出来的threadLocal对象，value是局部变量的值。



#### 线程池

##### 线程数的设置

	Nthreads = Ncpu * Ucpu * (1 + W/C)
	Ncpu 是 CPU 内核数，通过 Runtime.getRuntime().availableProcessors() 得到的值
	Ucpu 是期望的 CPU 的使用率(介于 0 与 1 之间)
	W/C 是等待时间与任务执行时间的比率
	
##### ThreadPoolExecutor工作原理
1. 查看当前Executor运行状态，如果不是RUNNING状态，将直接拒绝新任务。否则进入步骤2。
2. 查看当前运行线程的数量，如果数量少于核心线程数，将直接创建新的线程执行该任务。否则进入步骤3。
3. 将该任务添加到阻塞队列，等待核心线程执行完上一个任务再来获取。如果添加到阻塞队列失败，进入步骤4。
4. 尝试创建一个非核心线程执行该任务，前提是线程的数量少于等于最大线程数。如果失败，拒绝该任务。

任务队列的拒绝策略：

1. CallerRunsPolicy：线程调用运行该任务的 execute 本身
2. AbortPolicy：jdk默认策略，处理程序遭到拒绝将抛出运行时，RejectedExecutionException
3. DiscardPolicy：不能执行的任务将被删除， 不抛异常
4. DiscardOldestPolicy：如果执行程序尚未关闭，则位于工作队列头部的任务将被删除，然后重试执行程序（如果再次失败，则重复此过程）


#### ForkJoinPool
ForkJoin是Java7提供的原生多线程并行处理框架，其基本思想是将大人物分割成小任务，最后将小任务聚合起来得到结果。它非常类似于HADOOP提供的MapReduce框架，只是MapReduce的任务可以针对集群内的所有计算节点，可以充分利用集群的能力完成计算任务。ForkJoin更加类似于单机版的MapReduce。

task要通过ForkJoinPool来执行，分割的子任务也会添加到当前工作线程的双端队列中，进入队列的头部。当一个工作线程中没有任务时，会从其他工作线程的队列尾部获取一个任务。
ForkJoin框架使用了工作窃取的思想（work-stealing），算法从其他队列中窃取任务来执行，通过此算法降低线程等待和竞争。

ForkJoin编程范式：将问题递归地分解为较小的子问题，并行处理这些子问题，然后合并结果，如：

~~~Java
if (my portion of the work is small enough)   
     do the work directly 
else   
     split my work into two pieces   
     invoke the two pieces and wait for the results
~~~

#### CAS如何实现同步（配合volatile），如何解决ABA问题
AtomicStampedReference和 AtomicMarkableReference是通过版本号（时间戳）来解决ABA问题的

#### volatile 如何保证可见性
1. 当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存。
2. 当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量


### Java集合包
#### concurrenthashmap 

##### 并行度
Concurrency Level决定segment的个数，segment必须是2的N次方，如果是Concurrency Level设置为14，segment就为16。segment和Concurrency Level的最大值为65535，即2的16次方。

一个segment相当于一个小的hashmap，不过ConcurrentHashMap中的HashEntry相对于HashMap中的Entry有一定的差异性：HashEntry中的value以及next都被volatile修饰，这样在多线程读写过程中能够保持它们的可见性，代码如下：

~~~Java
static final class HashEntry<K,V> {
        final int hash;
        final K key;
        volatile V value;
        volatile HashEntry<K,V> next;

~~~

##### 定位segment

使用Wang/Jenkins hash 的变种算法对元素的hashcode进行一次rehash。这样能使元素均匀分散到不同的segment，减小了锁竞争发生的概率。

~~~Java
public V get (Object key) {
	int hash = hash(key.hashCode());
	return segmentFor(hash).get(key, hash);
}
~~~

##### get操作

get操作使非常简单高效的，因为get过程不需要加锁，除非独到空值才会加锁重读。
不需要加锁，是因为get方法里将要使用的共享变量都定义成了volatile，保证了可见性，总能读到最新值。

##### 扩容（segment级别）
ConcurrentHashMap在**插入元素前**会判断segment里面的 HashEntry数组是否超过了threshold，如果超过了，就扩容。

这比HashMap更科学，因为HashMap是在插入之后进行的扩容，如果该元素插入之后没有元素再插入，相当于是进行了无意义扩容。

链表长度如果大于了8就会转换成红黑树，之后如果长度再减小到6就又会变成链表。
因为在链表过长，查询效率就低了。

##### size实现
先尝试2次不锁住所有segment来统计所有segment的大小，如果统计的过程中，容器的modCount变化了，则再采用加锁的方式来统计。（锁住所有的segment是非常低效的）

##### java8中的改动
jdk7中ConcurrentHashmap中，当长度过长碰撞会很频繁，链表的增改删查操作都会消耗很长的时间，影响性能。所以jdk8 中完全重写了concurrentHashmap,代码量从原来的1000多行变成了 6000多 行，实现上也和原来的分段式存储有很大的区别。

#### HashMap在多线程情况下
1. HashMap 的Entry链表可能会形成环形数据结构。一旦产生环形结构，Entry的next节点永远不为空，在插入新Entry的时候就会为了寻找链表的尾巴产生死循环。

2. 当某一个线程A通过iterator去遍历某集合的过程中，若该集合的内容被其他线程所改变了；那么线程A访问集合时，就会抛出ConcurrentModificationException异常

#### hash集合重写hashcode equals
hashCode方法用来定位，equals用来比较是否相等

1）HashMap和HashTable，ConcurrentHashMap实现原理，区别和扩容规则

1. 线程安全性和存取速度
	
		HasMap线程不安全，存取速度快；
		HashTable线程安全，但存取速度很慢，每次存取都加锁；
		ConcurrentHashMap是锁分段技术，既达到了线程安全，又兼顾存取速度。
	
2. 初始化和扩容

		HashMap初始化是16，负载因子为0.75，以原容量2倍的方式扩容；
		HashTable初始化容量为11，负载因子为0.75，以原容量*2+1的方式扩容；
		ConcurrentHashMap初始化容量是16，负载因子0.75，扩容很复杂。

2）Vector，ArrayList和LinkedList实现原理和区别，以及扩容规则

1. ArrayList和Vector都是基于数组实现的，LinkedList是基于链表实现的。数组方便于查找，链表方便于插入和删除。
2. Vector线程安全，但操作速度慢。LinkedList和ArrayList线程不安全，操作速度快。
3. ArrayList可以插入重复值且有序，HashSet不能插入重复值且无序，是使用HashMap的keySet来实现的，value用了一个new Object()这种类型。
4. 扩容ArrayList以1.5倍的方式进行扩容，Vector以2倍的形式扩容。LinkedList无需扩容，HashSet根据HashMap的扩容规则。

3）TreeMap用什么实现的，用来作为有序的Map和LinkedList孰优孰劣
TreeMap是用红黑树来实现的，用来排序比LinkedList要好，无论是查找还是排序，时间复杂度较低。

红黑树的5个性质：
1、节点要么红，要么黑
2、根节点为黑
3、NIL（叶节点）为黑，这条没什么卵用
4、红节点的两个子节点为黑
5、从某节点到各叶节点的路径上的黑节点数相等

### Java8的新特性
#### CompletableFuture
CompletableFuture类实现了CompletionStage和Future接口。Future是Java 5添加的类，用来描述一个异步计算的结果，但是获取一个结果时方法较少,要么通过轮询isDone，确认完成后，调用get()获取值，要么调用get()设置一个超时时间。但是这个get()方法会阻塞住调用线程，这种阻塞的方式显然和我们的异步编程的初衷相违背。
为了解决这个问题，JDK吸收了guava的设计思想，加入了Future的诸多扩展功能形成了CompletableFuture。

处理异常用handle。

#### stream， functional programming




### nio
netty epoll

### rpc
avro

### 堆排序


### 一个Http请求
DNS域名解析 –> 发起TCP的三次握手 –> 建立TCP连接后发起http请求 –> 服务器响应http请求，浏览器得到html代码 –> 浏览器解析html代码，并请求html代码中的资源（如javascript、css、图片等） –> 浏览器对页面进行渲染呈现给用户

### Spring的事务



## 框架基础
### SpringMVC工作机制

### SpringMVC的AOP实现原理

Spring AOP使用动态代理技术在运行期织入增强代码。使用两种代理机制：基于JDK的动态代理（JDK本身只提供接口的代理）；基于CGlib的动态代理。

1. JDK的动态代理主要涉及java.lang.reflect包中的两个类：Proxy和InvocationHandler。其中InvocationHandler只是一个接口，可以通过实现该接口定义横切逻辑，并通过反射机制调用目标类的代码，动态的将横切逻辑与业务逻辑织在一起。而Proxy利用InvocationHandler动态创建一个符合某一接口的实例，生成目标类的代理对象。（只能为接口创建代理实例）
2. CGLib采用底层的字节码技术，为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类的调用方法，并顺势织入横切逻辑。

### SpringMVC 的请求过程，一个Controller是单例还是多实例

1. DispatcherServlet前端控制器接收发过来的请求，交给HandlerMapping
2. HandlerMapping根据请求路径找到相应的HandlerAdapter（HandlerAdapter就是那些拦截器或Controller）
3. HandlerAdapter处理一些功能请求，返回一个ModelAndView对象（包括模型数据、逻辑视图名）
4. ViewResolver视图解析器，先根据ModelAndView中设置的View解析具体视图
5. 然后再将Model模型中的数据渲染到View上


Controller和servlet一样是单例（性能），不要在Controller里面定义成员变量，如果非要这样做，把scope改成prototype。

## 数据库基础
### 事务隔离级别
数据库事务的隔离级别有4个，由低到高依次为

1. Read uncommitted(未授权读取、读未提交)、
2. Read committed（授权读取、读提交）、
3. Repeatable read（可重复读取）、
4. Serializable（序列化），

这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。

![upload images](http://upload-images.jianshu.io/upload_images/1807893-6d7c5c78a71fcc09?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

问题的提出：
之所以提出事务隔离级别，是因为在一个事务执行过程中，可能会出现以下几种情况:

1、更新丢失
两个事务都同时更新一行数据，一个事务对数据的更新把另一个事务对数据的更新覆盖了。这是因为系统没有执行任何的锁操作，因此并发事务并没有被隔离开来。
2、脏读
一个事务读取到了另一个事务未提交的数据操作结果。
3、不可重复读（Non-repeatable Reads）：一个事务对同一行数据重复读取两次，但是却得到了不同的结果。
包括以下情况：
（1） 虚读：事务T1读取某一数据后，事务T2对其做了修改，当事务T1再次读该数据时得到与前一次不同的值。
（2） 幻读（Phantom Reads）：事务在操作过程中进行两次查询，第二次查询的结果包含了第一次查询中未出现的数据或者缺少了第一次查询中出现的数据（这里并不要求两次查询的SQL语句相同）。这是因为在两次查询过程中有另外一个事务插入数据造成的。

下面介绍一下这几种事务隔离级别的区别以及可能出现的问题：

**Read uncommitted(未授权读取、读未提交)**：
如果一个事务已经开始写数据，则另外一个事务则不允许同时进行写操作，但允许其他事务读此行数据。该隔离级别可以通过“排他写锁”实现。
避免了更新丢失，却可能出现脏读。也就是说事务B读取到了事务A未提交的数据。

**Read committed（授权读取、读提交）**：
读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。
该隔离级别避免了脏读，但是却可能出现不可重复读。事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。

**Repeatable read（可重复读取）**：
读取数据的事务将会禁止写事务（但允许读事务），写事务则禁止任何其他事务。
避免了不可重复读取和脏读，但是有时可能出现幻读。这可以通过“共享读锁”和“排他写锁”实现。

**Serializable（序列化）**：
提供严格的事务隔离。它要求事务序列化执行，事务只能一个接着一个地执行，但不能并发执行。如果仅仅通过“行级锁”是无法实现事务序列化的，必须通过其他机制保证新插入的数据不会被刚执行查询操作的事务访问到。
序列化是最高的事务隔离级别，同时代价也花费最高，性能很低，一般很少使用，在该级别下，事务顺序执行，不仅可以避免脏读、不可重复读，还避免了幻像读。

隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed。它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读和第二类丢失更新这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。

大多数数据库的默认级别就是Read committed，比如Sql Server , Oracle。
Mysql的默认隔离级别就是Repeatable read。

### 数据库连接池,锁性等
### Spring事务传播性

  1. Propagation.REQUIRED
方法被调用时自动开启事务，在事务范围内使用则使用同一个事务，否则开启新事务。       
  2. Propagation.REQUIRES_NEW
无论何时自身都会开启事务
  3. Propagation.SUPPORTS
自身不会开启事务，在事务范围内则使用相同事务，否则不使用事务。如果有开启本地事务（比如自动提交），会使用本地事务。
  4. Propagation.NOT_SUPPORTED
自身不会开启事务，在事务范围内使用挂起事务，运行完毕恢复事务
  5. Propagation.MANDATORY
自身不开启事务，必须在事务环境使用否则报错
  6. Propagation.NEVER
自身不会开启事务，在事务范围使用抛出异常
  7. Propagation.NESTED 如果一个活动的事务存在，则运行在一个嵌套的事务中. 如果没有活动事务, 则按TransactionDefinition.PROPAGATION_REQUIRED 属性执行。需要JDBC3.0以上支持。

[Spring事务传播特性实例解析](http://blog.csdn.net/liovey/article/details/14149137)

### 数据库垮库一致性

### 数据库死锁的问题，一个删除昨天一个删除今天的，怎么死锁的。

## 设计模式
### JDK里面的设计模式

适配器模式：将一个接口适配到另一个接口，Java I/O中InputStreamReader将Reader类适配到InputStream，从而实现了字节流到字符流的准换。

装饰者模式：保持原来的接口，增强原来有的功能。
FileInputStream 实现了InputStream的所有接口，BufferedInputStreams继承自FileInputStream是具体的装饰器实现者，将InputStream读取的内容保存在内存中，而提高读取的性能。

java.util.Collections#synchronizedList(List)


1.Singleton（单例） 
作用：保证类只有一个实例；提供一个全局访问点
JDK中体现：
（1）Runtime
（2）NumberFormat

2.Factory（静态工厂） 
作用：
（1）代替构造函数创建对象
（2）方法名比构造函数清晰
JDK中体现：
（1）Integer.valueOf
（2）Class.forName

12.Flyweight（享元）
作用：共享对象，节省内存
JDK中体现：
（1）Integer.valueOf(int i)；Character.valueOf(char c)
（2）String常量池

## 工具
### redis cluster和shardedJedis
ShardedJedis通过一致性hash实现的的分布式缓存，是在客户端做的路由。sharded采用的hash算法：MD5 和 MurmurHash两种；默认采用64位的MurmurHash算法；每一个节点分配有160个虚拟节点，以达到数据平均分配的效果（持怀疑态度，生产上面6个节点，经常一两个内存占用特别高）。

redis cluster是通过预先分配好集群实例所负责的slot来达到路由的效果。

### redis初始设置大概有哪些（单实例模式）
bind *
maxmemory 3221225472
maxmemory-policy allkeys-lru


### Redis，在项目里面承担了核心缓存，选择的持久化方式是什么？
redis提供了多种不同级别的持久化方式用于crash恢复：一种是RDB,另一种是AOF.当 Redis 启动时， 如果 RDB 持久化和 AOF 持久化都被打开了， 那么程序会优先使用 AOF 文件来恢复数据集， 因为 AOF 文件所保存的数据通常是最完整的。

* **rdb**:
	fork一个进程，遍历hash table，利用copy on write，把整个db dump保存下来。
save, shutdown, slave 命令会触发这个操作。
粒度比较大，如果save, shutdown, slave 之前crash了，则中间的操作没办法恢复。
	
	优点：写性能高于aof 25%的样子; RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快;

* **aof**:
	把写操作指令，持续的写到一个类似日志文件里。（类似于从postgresql等数据库导出sql一样，只记录写操作）
粒度较小，crash之后，只有crash之前没有来得及做日志的操作没办法恢复。

### Redis 持久化的时候可能会因为内存不足，fork失败

当一个redis实例占用了4G内存，fork一个进程就需要申请另外4G，但是实际上只有2G，就会fork失败。
可以调整内核参数，vm.overcommit_memory=1
这样内核就会假装自己还有很多内存。


### Redis的内存废弃策略 
我们公司用的是volatile-lru。

* noeviction：当内存使用达到阈值的时候，所有引起申请内存的命令会报错。
* allkeys-lru：在主键空间中，优先移除最近未使用的key。
* volatile-lru：在设置了过期时间的键空间中，优先移除最近未使用的key。
* allkeys-random：在主键空间中，随机移除某个key。
* volatile-random：在设置了过期时间的键空间中，随机移除某个key。
* volatile-ttl：在设置了过期时间的键空间中，具有更早过期时间的key优先移除。


### redis高并发的key怎么处理?
可以从两个方面讲：

1. 高并发的情况下，如何防止大量线程穿特到持久层去，系统负载升高，可能会崩溃。
	
	```
	解决方案：
	* 如非必要，或者说hot key不会变化，可以直接不expire
	* 单机上面可以在加载方法上面加锁，分布式的环境下可以使用分布式锁（性能低，不推荐）
	* 可以单独写一个线程来定时刷新这些key，interval < expire
	```
2. hot key 导致了负载不均衡，大量读写操作路由到单个实例上面去了，可能会把这个实例压垮。

	```
	* 拆分复杂数据结构：如果当前key的类型是一个二级数据结构，例如哈希类型。如果该哈希元素个数较多，可以考虑将当前hash进行拆分，这样该热点key可以拆分为若干个新的key分布到不同Redis节点上，从而减轻压力。
	* 迁移热点key：以RedisCluster为例，可以将热点key所在的slot单独迁移到一个新的Redis节点上，但此操作会增加运维成本。
	* 本地缓存加通知机制：可以将热点key放在业务端的本地缓存中，因为是在业务端的本地内存中，处理能力要高出Redis数十倍，但当数据更新时，此种模式会造成各个业务端和Redis数据不一致，通常会使用发布订阅机制来解决类似问题。
	```


### MQ如何保证顺序性
就Kafka而言，顺序性只存在于partition中，如果程序有严格的时序要求的话，就需要传固定的key值。Kafka对于消息中没有包含key值的使用round-robin，如果传了key值hash取partition 数的余数来获得partition 序号。

~~~Java
public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        int numPartitions = partitions.size();
        if (keyBytes == null) {
            //round-robin, get the auto increment int value.
            int nextValue = nextValue(topic);
            List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);
            if (availablePartitions.size() > 0) {
                int part = Utils.toPositive(nextValue) % availablePartitions.size();
                return availablePartitions.get(part).partition();
            } else {
                // no partitions are available, give a non-available partition
                return Utils.toPositive(nextValue) % numPartitions;
            }
        } else {
            // hash the keyBytes to choose a partition
            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
        }
    }
~~~  
	    
### hadoop hdfs hive hbase


### Redis 原理
### Kafka原理，JMS AMPQ
    

## 网络
### 端口的个数
因为TCP/IP协议中规定数据包的包头中端口号的长度就是16个二进制位，二进制的十六个1，换算成十进制就是65535

### tcp 三次握手四次挥手
tcpdump + wireshark

### backlog
	backlog = 未完成三次握手队列size +  已经完成三次握手队列size
backlog参数设置既可以在linux内核参数设置(修改文件/etc/sysctl相关参数)，也可以在socket系统调用listen函数时设置(第二个参数)，这二者区别是，前者为全局性的，影响所有socket，后者为局部性的，影响当前socket。	
当server调用accept时，从已完成(三次握手)队列中的头部取出一个socket连接给进程，以下是变化过程。

其中backlog用来指定完成队列的大小，实际上系统用 min(backlog, somaxconn) + 1 来最终确定的。（somaxconn == cat /proc/sys/net/core/somaxconn  默认128）

未完成队列的大小有max(64, /proc/sys/net/ipv4/tcp_max_syn_backlog 默认1024) 来指定的。


这里服务器需要防tcp_flood 攻击（SYN洪水攻击）


socket = serverSocket.accept();// 从已完成队列中取出一个连接，如果没有则等待


### time-wait

主动关闭方调用完close，但被动方还没有调用close时，称为半关闭状态，这种时候，只能接收数据，不能在发送数据了。


被动关闭方，最后的状态是closed，而主动关闭方最后是timed_wait状态，timed_wait状态需要等待2MSL才会释放，在此期间应用程序是无法再次使用同一个插口的。
优化：建议不要在服务器端主动关闭连接，以节省不必要的timed_wait状态。

如果在服务端发现了很多time_wait状态链接，那就需要检查客户端代码是否主动close了链接。


## Linux 基础

常用指令

uptime、vmstat、mpstat、sar、ps、top、pidstat、perf

pidstat：可以查看到单个进程的cpu和io使用情况
mpstat： mpstat 不但能查看所有CPU的平均信息，还能查看指定CPU的信息。
sar： 与mpstat 一样，不但能查看CPU的平均信息，还能查看指定CPU的信息

vmstat：只能查看所有CPU的平均信息；查看cpu队列信息；
iostat: 只能查看所有CPU的平均信息。

如果上线时出了问题，先用jmap、jstack保留现场，然后做回滚。

## 性能调优
性能调优同样遵循 2-8 原则，80%的性能问题是由 20%的代码产生的，因此优化关键代码事半功倍。同时，对性能的优化要做到按需优化，过度优化可能引入更多问题。对于 Java 性能优化，不仅要理解系统架构、应用代码，同样需要关注 JVM 层甚至操作系统底层。总结起来主要可以从以下几点进行考虑：

### 基础性能的调优
这里的基础性能指的是硬件层级或者操作系统层级的升级优化，比如网络调优，操作系统版本升级，硬件设备优化等。比如 F5 的使用和 SDD 硬盘的引入，包括新版本 Linux 在 NIO 方面的升级，都可以极大的促进应用的性能提升；

### 数据库性能优化
包括常见的事务拆分，索引调优，SQL 优化，NoSQL 引入等，比如在事务拆分时引入异步化处理，最终达到一致性等做法的引入，包括在针对具体场景引入的各类 NoSQL 数据库，都可以大大缓解传统数据库在高并发下的不足；

### 应用架构优化
引入一些新的计算或者存储框架，利用新特性解决原有集群计算性能瓶颈等；或者引入分布式策略，在计算和存储进行水平化，包括提前计算预处理等，利用典型的空间换时间的做法等；都可以在一定程度上降低系统负载；

### 业务层面的优化
技术并不是提升系统性能的唯一手段，在很多出现性能问题的场景中，其实可以看到很大一部分都是因为特殊的业务场景引起的，如果能在业务上进行规避或者调整，其实往往是最有效的。
